\documentclass{article}
\usepackage[utf8]{inputenc}
%----------Packages----------
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
% \usepackage{amsrefs}
%\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{mathtools}
%\usepackage{stmaryrd}
%\usepackage[all]{xy}
\usepackage[mathcal]{eucal} % changes meaning of \mathcal
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{verbatim} %%includes comment environment
\usepackage{fullpage} %%smaller margins
\usepackage{indentfirst} %%indent first paragraphs after sections
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,      
    urlcolor=cyan,
}
\urlstyle{same}
\usepackage{graphicx}
\graphicspath{ {./img/} }
\usepackage{bm}
\usepackage{calc}

% \usepackage[
% backend=biber,
% style=annotate,
% bibstyle=annotate,
% sorting=nty,
% maxbibnames=20,
% maxcitenames=2,
% giveninits = true,
% terseinits=true,
% uniquename=false
% ]{biblatex}
\usepackage[authordate,backend=biber,isbn=false,annotation]{biblatex-chicago}
\addbibresource{ref.bib}
\DeclareNameAlias{sortname}{family-given}
\renewcommand*{\revsdnamepunct}{}

\usepackage{listings}
\usepackage{ulem}

% \usepackage{lineno}
% \linenumbers

%----------Commands----------
%%penalizes orphans
\clubpenalty = 9999
\widowpenalty = 9999

%% blackboard bold math capitals
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bbZ}{\mathbb{Z}}

\renewcommand{\phi}{\varphi}
\renewcommand{\emptyset}{\O}

\renewcommand{\_}[1]{\underline{ #1 }}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\DeclareMathOperator{\ext}{ext}
\DeclareMathOperator{\bridge}{bridge}

% Start for Sheet 14
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}

\newcommand{\G}{\mathcal{G}}

% End for Sheet 14

\newcommand{\head}[1]{
	\begin{center}
		{\large #1}
		\vspace{.2 in}
	\end{center}
	
	\bigskip 
}
\newcommand{\hint}[2][Hint]{
	
	(#1: #2)
}

%----------Theorems----------

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{examples}[theorem]{Examples}
\newtheorem{example}[theorem]{Example}

\newtheorem{axiom}{Axiom}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{definition*}{Definition}
\newtheorem{nondefinition}[theorem]{Non-Definition}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{warning}[theorem]{Warning}
\newtheorem{question}[theorem]{Question}

% \numberwithin{equation}{subsection}

\linespread{2}

%----------Editting----------

%\newcommand{\hide}[1]{} % for student version
%\newcommand{\com}[1]{} % for student version
%\newcommand{\meta}[1]{} % for removing meta comments in the script

\newcommand{\hide}[1]{{\color{red} #1}} % for instructor version
\newcommand{\com}[1]{{\color{blue} #1}} % for instructor version
\newcommand{\meta}[1]{{\color{green} #1}} % for making notes about the script that are not intended to end up in the script
%%--- sheet number for theorem counter
% Start for Sheet 14
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\T}{\mathscr{T}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Vrest}{V_{rest}}
\newcommand{\Vth}{V_{th}}
\newcommand{\tref}{\tau_{ref}}
% End for Sheet 14
\setcounter{theorem}{1}
\setcounter{equation}{1}

% \bibliographystyle{apalike}
% \setcitestyle{authoryear, open={((},close={))}

\title{}
\author{Amelia Simonoff}
\date{MONTH DD 2022}


\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
            
        \Huge
        \textbf{Face the Music}
            
        \vspace{0.5cm}
        \LARGE
            
        \vspace{1.5cm}
            
        \textbf{Amelia Simonoff}
        
        \textbf{DD/MM/2022}
            
        \vfill
            
        A thesis presented for the degree of\\
        \textbf{Bachelor of Arts in Music} \\
        At the
        
        \vspace{0.8cm}
            
        \includegraphics[width=0.9\textwidth]{UChicago_College_Horizontal_Color_RGB.png}

        \Large
            
    \end{center}
\end{titlepage}

\section{Abstract}

Since time immemorial, music has both elicited and strengthened emotions. Normally, music elicits emotions, from the performer in the listener. However, little has been done going the other direction: Emotions do not usually create music. Emotions are often used during the composition and creation of music, but that is still through and intermediate: the composer. In this project, I wanted to go the other way: What would happen if the emotions of the listener affected the music?

To that end, I use facial recognition technology to read a user's emotions, as current facial recognition technology has progressed to the point that it can be done in nearly real-time. In this project, I use the Python library \href{https://pypi.org/project/deepface/}{DeepFace} to read peoples' emotions in real-time, and then the Spotify \href{https://developer.spotify.com/documentation/web-api/reference/}{Web API} and \href{https://developer.spotify.com/documentation/web-playback-sdk/reference/}{Web Playback SDK} to play music related to those emotions. I set up this project on a Raspberry Pi, continually running a Python script that reads data from the camera and then pulls data from Spotify. 

I then set up the Raspberry Pi in the Media Arts, Data, and Design (MADD) Center at the University of Chicago. 


\section{Background}

From the first songs around a campfire, to soaring symphonies in concerthalls, to twenty-thousand strong rock band audiences, to sad songs after a breakup, music guides us, drives us, and affects who we are. It has always been part of the human experience, and music is often used to change or strengthen one's mood (\cite{huron2011}).

Music is incredibly prevalent around us in the modern day, and it is a language that is able to communicate emotional ideas succinctly, which speech is unable to do (\cite{henson1977}). However, music does does not always elicit emotions (\cite{16juslin2019}). The emotions that it does elicit, however, are usually basic emotions, such as joy, love, happiness, fear, anger, sorrow, and neutral (\cite{sundberg1983}). The ways in which it does so is affected by affected the current internal state of the listener (\cite{harrer1977}). 

Patrik N. Juslin has created a model for how music elicits emotions in an individual, known as the BRECVEMA model: Brainstem reflex, Rhythmic entrainment, Evaluative conditioning, Contagion, Visual imagery, Episodic memory, Musical expectancy, and Aesthetic judgement (\cite{17juslin2019}). Each of these mechanisms affects the effected emotion differently: for example, music that stimulates the brainstem (such as sudden loud sounds) is more likely to induce arousal or surprise, while music that triggers episodic memory can induce any possible emotion, but especially nostalgia and longing (\cite{25juslin2019}). Of course, there are also other mechanisms with which music can elicit emotions, such as cognitive goal appraisal. 

In fact, music listeners report that experiencing negative emotions, such as sadness, anger, or fear, is often pleasurable (\cite{32juslin2019}). However, music can only induce emotions over time: one note, out of context, cannot create an emotional state all on its own (\cite{schubert2001}). Also, the emotions that music induces are affected by the society and culture of the listener, which people often forget. Music also induces stronger emotions when it can be controlled, such as in private spaces or through busking (\cite{oneill2001}). 

I wanted to, instead, focus on the other direction: what if we create music based on an emotion, instead of the other way around?

For that, I needed a way to read the emotions of users in near real-time, and then a way to either synthesize music of a certain emotion or pull from an already-existing database. I knew that facial recognition software could accomplish the first step, and so I chose to use a pre-trained model.

Facial recognition software has been available for the past DECADE (citation), and its implementations in the modern day are not without controversy. However, much research has been done on facial recognition, and current implementations are incredibly lightweight, especially using pre-trained models. 

Unfortunately, real-time music synthesis is not yet available. While artificial-intelligence based musical synthesis has already passed the Turing Test (citation), the models are not quite versatile enough yet to produce a continuous stream of music with changing input parameters (i.e., the emotion of the user). As such, I decided to use a corpus of music that is already labeled, which is found in many streaming services, such as \href{https://www.apple.com/apple-music/}{Apple Music} or \href{https://www.spotify.com/us/}{Spotify}.

The art of providing recommendations from a previous song is a million-dollar problem in the music streaming industry: customer retention is, as with all other entertainment services, paramount. As such, services such as Apple Music, Pandora, or Spotify, all have well-curated algorithms for their musical recommendations, which are especially fine-tuned for commercial music (\cite{drott2018}). As such, I chose to use one of those algorithms for obtaining recommendations for my own project. By feeding the algorithm the genre preferences of the user, their top songs, and their emotion (see \hyperref[Methods]{Methods}), I receive musical suggestions that the user is likely to enjoy which also correspond to their current emotion.

Spotify, the most-used music streaming service in the world (\cite{mulligan2022}), has many playlists with emotional labels, such as "Happy Beats" or "Sad Piano", etc.. However, Spotify does not label their music based on emotion outright, and instead classifies music by a large variety of parameters (see \hyperref[Methods]{Methods}). 

\section[Significance Statement]{Significance Statement}

Music is cool facial recognition is dope something about fronto-temporal dementia

\section[Methods]{Methods}
\label{Methods}

This project is running on a Python backend with a Node.js environment communicating to a React application to output the music to a browser. The general overview of the project is outlined in \hyperref[fig1]{Figure 1}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{FtM Project.drawio.png}
    \caption{High-level overview of the project. Each block is a submodule, each arrow is a communication pipeline. Left half written in Python, right half written in JavaScript.}
    \label{fig1}
\end{figure}

All Python processes (known as workers for \href{https://docs.python.org/3/library/multiprocessing.html}{multiprocessing}) run asynchronously and communicate using a \href{https://docs.python.org/3/library/queue.html}{Queue} datastructure. This is because the time for each processing step in the pipeline is variable, but results must be displayed at the same time. 

\subsection[Video Input]{Video Input}

I use a Python environment running \href{https://pypi.org/project/opencv-python/}{OpenCV} to pull video data from the webcam. Each frame is passed to \href{https://pypi.org/project/deepface/}{DeepFace}, which is a wrapper for multiple state-of-the-art emotion recognition models.

\subsection[Emotion Analysis]{Emotion Analysis}

I rewrote the available open-source code for real-time analysis of incoming video frames. 

Each frame is classified by the probability that it is one of the following emotions: \texttt{angry, disgusted, scared, happy, sad, surprised}, and \texttt{neutral}. Then, the most likely emotion is chosen as the \texttt{dominant emotion}, which is then sent for downstream processing.

\subsection[Video Output]{Video Output}

Each video frame is rendered, and a user's emotion, if available, is displayed on the frame. Pulling and rendering video frames takes significantly less time than analyzing an emotion in a frame, and so often multiple video frames are displayed with the same analyzed emotion.

\subsection[Music Recommendations]{Music Recommendations}

Parsed emotions are also passed to a third worker, which communicates with Spotify through the \href{https://developer.spotify.com/documentation/web-api/}{Spotify Web API}. Upon initialization, the worker authenticates the user using the Spotify OAuth2 service, and then queries the API for the user's top songs and artists. The worker then makes a list of the genres of the user's top songs, and compares it with the available genres for recommendation, which is used as a mask. 

After initialization, the worker loads the parameters for the recommendation request for each emotion. Every ten seconds, the worker queries Spotify for a list of recommendations given a random number of user top songs, artists, and genres as seeds, as well as the parameters given by the current emotion. The worker then plays a randomly chosen song from the returned list of suggestions, starting between one-sixth and one-third of the way through the song. I chose these values because they are close to the start of the song but are past any lead-ins or introns, which might not accurately represent the emotion codified by the song.

The tunable parameters that I chose for each emotion for the \href{https://developer.spotify.com/documentation/web-api/reference/#/operations/get-recommendationsrecommendation}{recommendations query} from Spotify are as follows. Each parameter has a maximum, minimum, and target value (e.g. \texttt{min\char`_acousticness}, \texttt{max\char`_danceability}, \texttt{target\char`_valence} ): \texttt{acousticness, danceability, energy, instrumentalness, liveness, loudness, popularity, speechiness, tempo}, and \texttt{valence}. See \href{https://github.com/as4mo3/face-the-music/blob/master/params.py}{params.py} for the tuned parameters for each emotion.

The emotions of the currently-playing music are also passed to the video output, where they are rendered on each frame.

\subsection[Server Backend and Frontend]{Server Backend and Frontend}

Concurrently, a Node.js process supporting a React application is running to display the \href{https://developer.spotify.com/documentation/web-playback-sdk/}{Spotify Web Playback SDK} in a browser. The backend once again authorizes the user, and the browser functions as a device for Spotify playback, which is used as the default playback destination for the Spotify worker. 

\subsection[Parameter Tuning]{Parameter Tuning}

Following the BRECVEMA model, discussed in \cite{17juslin2019}, different aspects encoded in music can stimulate different emotions through the stimulation of different physical processes (\cite{25juslin2019}, Table 25.1). The following list describes my vision for each emotion. The complete list of tuned parameters can be found in \href{https://github.com/as4mo3/face-the-music/blob/master/params.py}{params.py}. 

\begin{itemize}
    \item \textbf{Angry}: Anger, a basic emotion, is usually observed due to the stimulation of the amygdala in particular and limbic system in general. Furthermore, listeners can become physiologically entrained to the music, with their hearbeats and breathing changing due to the rhythms. As such, angry music must be loud and, ideally, have a beat at around 2Hz for physical entrainment (120 bpm). However, it also must be "heavy" enough to exude a low emotional valence, for which the \texttt{valence} parameter is ideal.
    \item \textbf{Disgusted}: Disgust, another basic emotion originating from the limbic system, is a primal avoidance reflex, which is usually triggered from a positive expectation not being met. Music that is disgusting must, as such, not meet the expectations of the listeners, in whatever sense. As such, I tuned the parameters to focus on discordant music with non-standard beats per minute.
    \item \textbf{Scared}: Fear is one of the most primal emotions. It can be elicited by the slow buildup of the music towards something unknown, with an increase in one's pulse and other physiological states. The parameters for this kind of music were tuned to low-\texttt{valence} and mid-\texttt{tempo}.
    \item \textbf{Happy}: An increase in excitement is observed in music with high rhythmic entrainment and tempo, usually thought of in a major key. For this, the \texttt{danceability} parameter was set to a maximum, as musical rhythms inducing a high level of motor responses (especially in a group setting) are perceived with a high emotional valence.
    \item \textbf{Sad}: Sad music is thought of as, mostly, slow and somber. As such, I tuned the parameters for this kind of music to be low-\texttt{valence} and low-\texttt{tempo}, with minimal amounts of \texttt{danceability}.
    \item \textbf{Surprised}: Surprised is a very basic reflex originatng from the brainstem. Sudden loud noises, key changes, or other unpredictable effects in the music can mark surprise. Notably, surprising music must have high-\texttt{valence}, as music with a low-\texttt{valence} would more likely be understood as fearful.
    \item \textbf{Neutral}: Neutral music is the hardest to define -- what one person may consider neutral may be the most exciting to another. As such, the parameters here were all tuned to be middle-of-the-road, targets of 50\% for most of them. 
\end{itemize}

\section[Code Availability]{Code Availability}

The code is available on \href{https://github.com/as4mo3/face-the-music/}{Github} under the GNU General Public License, version 3. 

\section[Observations]{Observations}
\label{Observations}

I observed people being more entertained with the emotional recognition portion of the project than with the music generation. Knowing that there are seven emotions, people would try to "unlock them all," which is not as easy as it sounds (disgust is notoriously hard to produce consistently). 

\section[Discussion and Conclusion]{Discussion and Conclusion}
\label{Discussion and Conclusion}

Music and emotions are, of course, indelibly linked. In this project, investigating how emotions affect music, I found that people are more interested in the analysis of their emotions than the music elicited by them. 

% \section[Conclusion]{Conclusion}
% \label{Conclusion}

% \section{References}

% add the Bibliography to the Table of Contents
\cleardoublepage
\ifdefined\phantomsection
  \phantomsection  % makes hyperref recognize this section properly for pdf link
\else
\fi

% include your .bib file
% \bibliography{ref}
\nocite{*} % include entire bibliography, including things that aren't cited
\printbibliography

% \pagebreak


% \section[Supplemental Material]{Supplemental Material}
% \label{Supplemental Material}

\end{document}